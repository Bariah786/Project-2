```python

import pandas as pd

*** Loading the data files ***
#Products
url= 'https://drive.google.com/file/d/1_7_0FUsjfKcyaCBfZlhrmNTctACf_6ob/view?usp=drive_link'
path = "https://drive.google.com/uc?export=download&id="+url.split("/")[-2]
products_df = pd.read_csv(path)

#Orders
url= 'https://drive.google.com/file/d/1oHwUzohlDAT161sfzjX5WVQfIZ1ZLQRd/view?usp=drive_link'
path = "https://drive.google.com/uc?export=download&id="+url.split("/")[-2]
orders_df = pd.read_csv(path)

#Orderlines
url= 'https://drive.google.com/file/d/1x6BI5Bsb-mLFO-O_gct7WF5UbU4s90Py/view?usp=drive_link'
path = "https://drive.google.com/uc?export=download&id="+url.split("/")[-2]
orderlines_df = pd.read_csv(path)

#Brands
url= 'https://drive.google.com/file/d/1ACHWXEnL9TxNWjrtWpxszV29YrJDNWa2/view?usp=drive_link'
path = "https://drive.google.com/uc?export=download&id="+url.split("/")[-2]
brands_df = pd.read_csv(path)

*** Creating copies ***
###Creating copies

products= products_df.copy()
orders= orders_df.copy()
orderlines= orderlines_df.copy()
brands= brands_df.copy()

*** Cleaning Dataframe PRODUCTS:***
products.info()
###Dropping duplicates

products.duplicated().value_counts(normalize= True)
products= products.drop_duplicates()

#Checking for duplicates.
products['sku'].nunique()           #10579
len(products['sku'])                #10580

"We still have any duplicate rows for the same product. So we need to find it out and drop."

#Let's check first
products.loc[products.duplicated(subset=['sku'],keep=False)]

#Drop it.
products.drop_duplicates(subset='sku',inplace=True)

###Dropping Null values

##1. Null values in desc.
"We gonna use description in further analyses so let's fill its null values with anything"

products.loc[products_df['desc'].isna()]
#Fill it with the 'name'
products.loc[products['desc'].isna(),'desc'] = products.loc[products['desc'].isna(),'name']

##2. Null values in price.
#% of Null values in price
(products['price'].isna().value_counts(normalize= True))*100

"Column price has 46 null values out of which I guess we can still fill some using the orderlines data frame. Using SKU we can find the Max
price of the product at which it has been sold."

#Fillin g the missing values of price
max_sku_prices = orderlines_df.groupby('sku')['unit_price'].max().reset_index()
merged_df= products.merge(max_sku_prices, on= 'sku', how= 'left') 
merged_df.loc[merged_df['price'].isna(),'price']= merged_df.loc[merged_df['price'].isna(),'unit_price']
products= merged_df.drop(columns= 'unit_price')

"Now we are left with 22 null values. In my opinion, we can drop them safely, as we have seen that they were not ordered anyhow."
#Dropping rows based on the remaining corrupted values.
products.dropna(subset= ['price'], inplace=True)

"We still have some null values in column 'type' but as per my understanding, we are not gonna use that column further in our analyses. So,
for now, ignoring it and not dropping rows (as they have other data) is a good idea."

###Changing type of data.

#Convert column 'price' into data type numeric
"It has few values with two decimal points so using simple 'to_numeric' will not work here. We have to get rid of one decimal point first."

#% of corrupted values
products['price'].str.count(r'\.').value_counts(normalize= True)

#Dropping the corrupted values
p2= products.loc[products['price'].str.count('\.')== 2].index
products= products.drop(p2)

#Converting 'price' to numeric
products['price']= pd.to_numeric(products['price'], errors='coerce')

# Correct the position of the decimal by moving it to one digit on the left.
products.loc[p2,'price']= products.loc[p2,'price']/10

###Changing data type of column Promo_Price from object to numeric.
#It has few values with two decimal points so using simple 'to_numeric' will not work here. We have to get rid of one decimal point first.

"I sorted data in promo_price (using the second idea from following) but skipped dropping rows based on corrupted values in promo_price
because we probably not be using this column in our further analyses. So, losing data based on it is pointless."

#Following is the code to sort values for 'promo_price'."

products['promo_price'].str.count('\.').value_counts()

*** First idea of cleaning and sorting column Promo_Price:***

#promo_price with zero decimal point
pp0= products.loc[products['promo_price'].str.count('\.')== 0, ['price', 'promo_price']]

#Promo_Price with 1 decimal point. 
pp1= products.loc[products['promo_price'].str.count('\.')== 1].index

#Moving a decimal point to one digit left.
products.loc[pp1,'promo_price']= products.loc[pp1,'promo_price'].astype(str)\
   .str.replace(r'(\d+)(\.)(\d+)', lambda m: m.group(1)[:len(m.group(1))-1] + '.' + m.group(1)[-1] + m.group(3), regex=True)     

#Promo_Price with 2 decimal points.  
pp2= products.loc[products['promo_price'].str.count('\.')== 2].index

#Removing the first decimal point.
products.loc[pp2,'promo_price']= products.loc[pp2,'promo_price'].astype(str)\
   .str.replace(r'(\d+)\.(\d+)\.(\d+)', lambda m: m.group(1) + m.group(2) + '.' + m.group(3), regex=True) 

#Moving the second decimal point to one digit left.
products.loc[pp2,'promo_price']= products.loc[pp2,'promo_price'].astype(str)\
   .str.replace(r'(\d+)(\.)(\d+)', lambda m: m.group(1)[:len(m.group(1))-1] + '.' + m.group(1)[-1] + m.group(3), regex=True) 

#Converting 'promo_price' to numeric
products['promo_price'] = round(pd.to_numeric(products['promo_price']),2)

#Check
(products.loc [round(products['promo_price']) > round(products['price']) , [('price'), ('promo_price')]].count() \
       / products.shape[0])*100

*** Second idea of cleaning and sorting column Promo_Price:***

#Removing first dot from values having 2 decimel points
pp2= products.loc[products['promo_price'].str.count('\.')== 2].index
products.loc[pp2,'promo_price']= products.loc[pp2,'promo_price'].astype(str)\
  .str.replace(r'(\d+)\.(\d+)\.(\d+)', lambda m: m.group(1) + m.group(2) + '.' + m.group(3), regex=True) 

#Converting 'promo_price' to numeric
products['promo_price']= pd.to_numeric(products['promo_price'], errors='coerce')

#Filtering out the corrupted values
mask= products.loc [round(products['promo_price']) > round(products['price'])].index

# Correct the position of the decimal by moving it to one digit on the left.
products.loc[mask,'promo_price']= round((products.loc[mask,'promo_price']/10),2)

#Check
((products.loc[round(products['promo_price']) > round(products['price'])]).count()/ len(products['promo_price']))*100

#% of still corrupted values
1.21

"I used till here only but in case to get rid of corrupted values following code can be used."

#To get rid of them
still_corrupted= (products.loc[round(products['promo_price']) > round(products['price'])]).index
products= products.drop(still_corrupted)

"The second idea makes more sense and all the promo prices are now either less or equal to the original price.
And in still corrupted values we just dropped 123 rows(1.21%)"

*** Cleaning Dataframe ORDERS:***

orders.info()

###Duplicates

orders.duplicated().sum()              #has no duplicates

###Droping Null values

#% of Null values
orders['total_paid'].isna().value_counts(normalize=True)       

#drop Null values
orders = orders.dropna(axis=0)   #only column 'total_paid' has null values

###Correcting the datatypes

#created_date should become datetime datatype
orders["created_date"] = pd.to_datetime(orders["created_date"])

***Cleaning Dataframe ORDERLINES:***
orderlines.info()

###Duplicates

orderlines.duplicated().sum()              #has no duplicates

###Correcting the datatypes

#date should be a datetime datatype
orderlines["date"] = pd.to_datetime(orderlines["date"])

#unit_price should be a float datatype

# Count the number of decimal points in the unit_price
orderlines['unit_price'].str.count("\.").value_counts()

#It is a string because it has a few values with two decimal points.

#Let's work out how much that is as a percentage of our total data.
mult_decimal_rows = (orderlines['unit_price'].str.count("\.")>1).value_counts(normalize= True)        #12.3%

"We'll try to save the corrupted values by comparing 'unit_price' from the orderlines to 'price' from the products."

# Boolean mask to find the orders that contain a price with multiple decimal points
multiple_decimal_mask = orderlines['unit_price'].str.count("\.") > 1

# Apply the boolean mask to the orderlines DataFrame. This way we can find the order_id of all the affected orders.
corrupted_order_ids = orderlines.loc[multiple_decimal_mask, 'id_order']

#Merge the dataframes
comparing= corrupted_order_ids.merge(products, on= 'sku', how= 'left')
comparing[['id_order','sku','price','unit_price']]

"By comparison we can see that this first decimal point is somehow read wrong. Unit_price 1.137.99 is actually could be 1137.99 because its
'orignal price' in products is 1219.00. Same for other values."

#SO, let's remove the first decimal from unit_price and save the data :)
#Removing first dot from values having 2 decimel points
pp2= orderlines.loc[orderlines['unit_price'].str.count('\.')== 2].index
orderlines.loc[pp2,'unit_price']= orderlines.loc[pp2,'unit_price'].astype(str).str.replace(r'(\d+)\.(\d+)\.(\d+)', lambda m: m.group(1) \
                                    + m.group(2) + '.' + m.group(3), regex=True) 

#Change 'unit_price' to numeric
orderlines['unit_price']= pd.to_numeric(orderlines['unit_price'], errors= 'coerce')

***Saving the clean data frames:***

from google.colab import files

#Clean dataframe of Products:
products.to_csv('products_cl.csv', index=False)
files.download('products_cl.csv')

#Clean dataframe of Orders
orders.to_csv('orders_cl.csv', index=False)
files.download('orders_cl.csv')

#Clean dataframe of Orderlines
orderlines.to_csv('orderlines_cl.csv', index=False)
files.download('orderlines_cl.csv')
